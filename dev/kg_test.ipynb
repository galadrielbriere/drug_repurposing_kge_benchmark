{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/galadriel/dr_benchmark/dev/my_data_redundancy.py:22: TqdmExperimentalWarning: Using `tqdm.autonotebook.tqdm` in notebook mode. Use `tqdm.tqdm` instead to force console mode (e.g. in jupyter console)\n",
      "  from tqdm.autonotebook import tqdm\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import os\n",
    "import argparse\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import yaml\n",
    "import random\n",
    "import numpy as np\n",
    "import torch\n",
    "import logging\n",
    "from torch import cat\n",
    "from kg_processing import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = parse_yaml('/home/galadriel/dr_benchmark/processed_kgs/minimal_kg/mixed_test/unpermuted/params.yaml')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load knowledge graph\n",
    "input_file = config[\"common\"]['input_csv']\n",
    "kg_df = pd.read_csv(input_file, sep=\"\\t\")[[\"my_x_id\", \"my_y_id\", \"relation\"]]\n",
    "kg_df = kg_df.rename(columns={'my_x_id': 'from', 'my_y_id': 'to', 'relation': 'rel'})\n",
    "\n",
    "if config[\"clean_kg\"][\"smaller_kg\"]:\n",
    "    logging.info(f\"Keeping only relations {config['clean_kg']['keep_relations']}\")\n",
    "    kg_df = kg_df[kg_df['rel'].isin(config[\"clean_kg\"]['keep_relations'])]\n",
    "\n",
    "kg = my_knowledge_graph.KnowledgeGraph(df=kg_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'kg' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m set_random_seeds(config[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcommon\u001b[39m\u001b[38;5;124m\"\u001b[39m][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mseed\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[0;32m----> 3\u001b[0m id_to_rel_name \u001b[38;5;241m=\u001b[39m {v: k \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m \u001b[43mkg\u001b[49m\u001b[38;5;241m.\u001b[39mrel2ix\u001b[38;5;241m.\u001b[39mitems()}\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m config[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mclean_kg\u001b[39m\u001b[38;5;124m\"\u001b[39m][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mremove_duplicates_triplets\u001b[39m\u001b[38;5;124m'\u001b[39m]:\n\u001b[1;32m      6\u001b[0m     logging\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRemoving duplicated triplets...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'kg' is not defined"
     ]
    }
   ],
   "source": [
    "set_random_seeds(config[\"common\"][\"seed\"])\n",
    "\n",
    "id_to_rel_name = {v: k for k, v in kg.rel2ix.items()}\n",
    "\n",
    "if config[\"clean_kg\"]['remove_duplicates_triplets']:\n",
    "    logging.info(\"Removing duplicated triplets...\")\n",
    "    kg = my_data_redundancy.remove_duplicates_triplets(kg)\n",
    "\n",
    "duplicated_relations_list = []\n",
    "\n",
    "if config['clean_kg']['check_synonymous_antisynonymous']:\n",
    "    logging.info(\"Checking for synonymous and antisynonymous relations...\")\n",
    "    theta1 = config['clean_kg']['check_synonymous_antisynonymous_params']['theta1']\n",
    "    theta2 = config['clean_kg']['check_synonymous_antisynonymous_params']['theta2']\n",
    "    duplicates_relations, rev_duplicates_relations = my_data_redundancy.duplicates(kg, theta1=theta1, theta2=theta2)\n",
    "    if duplicates_relations:\n",
    "        logging.info(f'Adding {len(duplicates_relations)} synonymous relations ({[id_to_rel_name[rel] for rel in duplicates_relations]}) to the list of known duplicated relations.')\n",
    "        duplicated_relations_list.extend(duplicates_relations)\n",
    "    if rev_duplicates_relations:\n",
    "        logging.info(f'Adding {len(rev_duplicates_relations)} anti-synonymous relations ({[id_to_rel_name[rel] for rel in rev_duplicates_relations]}) to the list of known duplicated relations.')\n",
    "        duplicated_relations_list.extend(rev_duplicates_relations)\n",
    "\n",
    "if config['clean_kg'][\"permute_kg\"]:\n",
    "    to_permute_relation_names = config['clean_kg'][\"permute_kg_params\"]\n",
    "    if len(to_permute_relation_names) > 1:\n",
    "        logging.info(f'Making permutations for relations {\", \".join([rel for rel in to_permute_relation_names])}...')\n",
    "    for rel in to_permute_relation_names:\n",
    "        logging.info(f'Making permutations for relation {rel} with id {kg.rel2ix[rel]}.')\n",
    "        kg = my_data_redundancy.permute_tails(kg, kg.rel2ix[rel])\n",
    "\n",
    "if config['clean_kg']['make_directed']:\n",
    "    undirected_relations_names = config['clean_kg']['make_directed_params']\n",
    "    relation_names = \", \".join([rel for rel in undirected_relations_names])\n",
    "    logging.info(f'Adding reverse triplets for relations {relation_names}...')\n",
    "    kg, undirected_relations_list = my_data_redundancy.add_inverse_relations(kg, [kg.rel2ix[key] for key in undirected_relations_names])\n",
    "        \n",
    "    if config['clean_kg']['check_synonymous_antisynonymous']:\n",
    "        logging.info(f'Adding created reverses {[rel for rel in undirected_relations_names]} to the list of known duplicated relations.')\n",
    "        duplicated_relations_list.extend(undirected_relations_list)\n",
    "\n",
    "logging.info(\"Splitting the dataset into train, validation and test sets...\")\n",
    "kg_train, kg_val, kg_test = kg.split_kg(validation=True)\n",
    "\n",
    "kg_train_ok, _ = verify_entity_coverage(kg_train, kg)\n",
    "if not kg_train_ok:\n",
    "    logging.info(\"Entity coverage verification failed...\")\n",
    "else:\n",
    "    logging.info(\"Entity coverage verified successfully.\")\n",
    "\n",
    "if config['clean_kg']['clean_train_set']:\n",
    "    logging.info(\"Cleaning the train set to avoid data leakage...\")\n",
    "    logging.info(\"Step 1: with respect to validation set.\")\n",
    "    kg_train = my_data_redundancy.clean_datasets(kg_train, kg_val, known_reverses=duplicated_relations_list)\n",
    "    logging.info(\"Step 2: with respect to test set.\")\n",
    "    kg_train = my_data_redundancy.clean_datasets(kg_train, kg_test, known_reverses=duplicated_relations_list)\n",
    "\n",
    "kg_train_ok, _ = verify_entity_coverage(kg_train, kg)\n",
    "if not kg_train_ok:\n",
    "    logging.info(\"Entity coverage verification failed...\")\n",
    "else:\n",
    "    logging.info(\"Entity coverage verified successfully.\")\n",
    "\n",
    "if config['clean_kg']['rel_swap']:\n",
    "    kg_train, kg_val, kg_test = specs_sets(kg_train, kg_val, kg_test, config)\n",
    "\n",
    "new_train, new_val, new_test = my_data_redundancy.ensure_entity_coverage(kg_train, kg_val, kg_test)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "benchmark",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
